相关链接：
    https://blog.csdn.net/qq_41997920/article/details/88693888






optimizer 优化器，用来保存当前的状态，并能够根据计算得到的梯度来更新参数
    给它一个可进行迭代优化的包含了所有参数列表params。
    然后，您可以指定程序优化特定的选项，例如学习速率，权重衰减等

Adam算法：(Adaptive Moment Estimation)本质上是带有动量项的RMSprop，
    它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。
    它的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。
    param 参数列表
    lr 学习率，学习率可以动态调整，调整方法暂时未知

Adamax:Adamax是Adam的一种变体，此方法对学习率的上限提供了一个更简单的范围

AdamW:AdamW是在Adam+L2正则化的基础上进行改进的算法

SparseAdam：针对稀疏张量的一种“阉割版”Adam 优化方法


BGD:梯度下降法，最原始和基础的算法
    它将所有的数据集都载入，计算它们所有的梯度，然后执行决策。（即沿着梯度相反的方向更新权重）
    优点是在凸函数能收敛到最小值。但显而易见的是，这方法计算量太大。
    假如我们的数据集很大的话，普通的GPU是完全执行不来的。
    还有一点，它逃不出鞍点，也容易收敛到局部最小值（也就是极小值）。

SGD:随机梯度下降法相比较BGD，其实就是计算梯度时根据的数据不同。
    SGD根据的是一整个数据集的随机一部分（上网有些介绍都是说根据每一个样本，但是现在都是采用一个随机的部分样本）。
    也就是说，它更新的速度比较频繁。
    因为我们随机选取的小批量数据(mini-batch)并不是太多，所以计算的过程也并不是很复杂。
    相比起BGD，我们在相同的时间里更新的次数多很多，也自然能更快的收敛。

ASGD:SGD的加速算法，但是有梯度延迟的缺点

SWA:随机权重平均，SGD的改进版本
    SWA相对于 SGD 来说有以下优点：
    1，不依赖学 习率的变化，设置恒定学习率的时候依然可以达到很好的收敛效果。
        而对于 SGD 而言，恒定学习率会导致收敛情况变差。
    2，收敛速度十分快，在原测试集上可 以在 150 个 epoch 就收敛得非常好，也非常平稳振荡幅度非常小。

lr_scheduler：torch.optim.lr_scheduler模块提供了一些根据epoch训练次数来调整学习率（learning rate）的方法。
    一般情况下我们会设置随着epoch的增大而逐渐减小学习率从而达到更好的训练效果。
    而torch.optim.lr_scheduler.ReduceLROnPlateau则提供了基于训练中某些测量值使学习率动态下降的方法。


Adagrad：Adagrad是解决不同参数应该使用不同的更新速率的问题。
    Adagrad自适应地为各个参数分配不同学习率的算法

Adadelta:Adadelta是对Adagrad的扩展，最初方案依然是对学习率进行自适应约束，但是进行了计算上的简化。
    Adagrad会累加之前所有的梯度平方，而Adadelta只累加固定大小的项，并且也不直接存储这些项，仅仅是近似计算对应的平均值。

LBFGS:我们知道算法在计算机中运行的时候是需要很大的内存空间的.就像我们解决函数最优化问题常用的梯度下降,
    它背后的原理就是依据了泰勒一次展开式.泰勒展开式展开的次数越多,结果越精确,
    没有使用三阶四阶或者更高阶展开式的原因就是目前硬件内存不足以存储计算过程中演变出来更复杂体积更庞大的矩阵.
    L-BFGS算法翻译过来就是有限内存中进行BFGS算法,L是limited memory的意思

RMSProp(均方根反向传播，Rprop的改进版）:RMSProp算法对梯度计算了 微分平方加权平均数。这种做法有利于消除了摆动幅度大的方向，
    用来修正摆动幅度，使得各个维度的摆动幅度都较小。另一方面也使得网络函数收敛更快。

Rprop（弹性方向传播）: